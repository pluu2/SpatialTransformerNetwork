{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import grad,jit,vmap\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as py\n",
    "#tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1= np.ones((28,28)) *0.5\n",
    "\n",
    "for i in range(2,22): \n",
    "  for j in range (2,22): \n",
    "    image1[i][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f873d204730>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "py.imshow(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= np.zeros((28,28))\n",
    "for i in range(2,22): \n",
    "  for j in range (2,22): \n",
    "    target[i][j]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f873d16bd00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "py.imshow(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This needs to be changed to store x,y, I will store the third diemsnion later. \n",
    "#each mask contains an x,y coordinate which is what the affine transformation works on.\n",
    "mask=np.zeros((784,2))\n",
    "i = 0\n",
    "for y in range(28):\n",
    "    for x in range(28): \n",
    "      mask[i][0]=y#-14\n",
    "      mask[i][1]=x#-14\n",
    "      #mask[i][2]=1\n",
    "      i+=1\n",
    "\n",
    "affine_matrix = np.array([[1,0],\n",
    "                [0,1]])\n",
    "\n",
    "\n",
    "#The transformation is performed on this mask, and then the sampling occurs on the actual 'feature map'\n",
    "#I believe this is what happens in Jaderberg et al.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_mask = np.dot (mask[2],affine_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=np.zeros((784,2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 784 into shape (784,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a40f9045dba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 784 into shape (784,2)"
     ]
    }
   ],
   "source": [
    "feature=image1.reshape(784,2) \n",
    "target=target.reshape(784,2)\n",
    "kernel = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network \n",
    "Sampling has to be something like\n",
    "\n",
    "$ V^C_i = \\sum^H_n\\sum^W_m U^C_{nm} max (0,1-|x^s_i-m|)max(0,1-|y^s_i-n|)$\n",
    "\n",
    "which has a very basic form of: \n",
    "$V = U \\tau F(x,y) $ with $U$ being the feature map\n",
    "\n",
    "$F(x,y)$ is the sampling, $U$ is the original feature map, and $\\tau$ is the affine matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THoughts on implementation: \n",
    "From the equation, I feel that what Jaderberg is getting at is basically a algorithm in which a feature map $U_{nm}^C$ is manipulated by affine matrix $\\tau$ or $k$....(ie: $V = Uk$) \n",
    "\n",
    "The entire algorithm is: $V=Uk(x-m;\\phi)(y-n;\\phi)$, which basically states, that the transformed image is a function of a feature map, tranformed by affine matrix k, only on the coordinates given by a sampling function parameterized by $\\phi$ \n",
    "\n",
    "In the code below, 'transformation_network' represents $UK(x-m;\\phi)(y-n;\\phi)$ \n",
    "the sampling network repreesents $V(x-m,y-n;\\phi)$ kind of . Like a sampling network that outputs whether or not those coordinates should be manipulated, via parameters $\\phi$\n",
    "\n",
    "\n",
    "In the code below for a given 28x28 image, there will be a matrix that stores all the feature map in the form of a [mn,dim] matrix. This matrix will undergo a affine transformation, with the resulting values being applied to the actual image, if the sampling network deems the actual pixel is worth applying to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the coordinates variable. \n",
    "coordinates=np.zeros((784,2))\n",
    "i = 0\n",
    "for y in range(28):\n",
    "    for x in range(28): \n",
    "      mask[i][0]=y\n",
    "      mask[i][1]=x\n",
    "      i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_map will store the actual color value of the image. This will be a [28,28] matrix. \n",
    "\n",
    "#affine_matrix is the actual transformation matrix. \n",
    "#coordinates is what I was calling a 'mask', and is a array of matrixes [784,2]. \n",
    "#I believe this will be constant. \n",
    "\n",
    "#The network decides if the pixel will have the affine trasnformation applied. \n",
    "def sampling_network(feature_map,parameters,bias):  #parameters will be 1,1  \n",
    "    \n",
    "    #The feature map then enters a sampling network, based on pixel value. \n",
    "    layer1=jnp.dot(feature_map,parameters[0].T) + bias[0]  #(32,1) \n",
    "    layer1=jax.nn.relu(layer1) \n",
    "    layer2=jnp.dot(layer1,parameters[1].T)+bias[1]  #(1,32)\n",
    "    layer2=jax.nn.sigmoid(layer2) \n",
    "    return layer2 #This outputs for a single pixel and would have to VMAP to the entire image\n",
    "\n",
    "#This function takes the affine_matrix and transforms the 784,2 array of vectors. \n",
    "# in the code below I have used the sampling network here, but i am debating whether\n",
    "#or not to just apply this during the transfrmation stage.  \n",
    "#I realized that I will do this at the level of trasnformation as the sampling_network examines\n",
    "#the pre-transformation image. \n",
    "\n",
    "def transformation_network (affine_matrix,coordinates,feature_map,parameters,bias): \n",
    "    #This is basically saying \"perform the affine matrix, and apply it whether or not the sampling _networ\n",
    "    #says to do it. \"\n",
    "    output= np.dot (coordinates,affine_matrix.T) * sampling_network(feature_map[coordinates[0]][coordinates[1]],parameters,bias)\n",
    "    return output #This should be a 784,2 output. \n",
    "\n",
    "def forward(affine_matrix,coordinates,feature_map,parameters,bias): \n",
    "    #First take the image and process it into the sampling_network. \n",
    "    #output will contain a 784,2 output, which has been trasnformed and can be applied \n",
    "    #to the actual image (feature_map) \n",
    "    output=transformation_network(affine_matrix,coordinates,feature_map,parameters,bias)\n",
    "    return output\n",
    "\n",
    "def complete_image(feature_map,transformation):\n",
    "    new_image=np.zeros((28,28))\n",
    "    feature_map=feature_map.flatten()\n",
    "    for i in range(784): \n",
    "        new_image[np.int(transformation[i][0])][np.int(transformation[i][1])] = feature_map[i]\n",
    "    return new_image \n",
    "\n",
    "\n",
    "def MSE_error(pixels_,target,parameters,bias): \n",
    "  pred = forward(pixels_,parameters,bias) \n",
    "  error = 0.5*(target-pred)**2 #sum over image\n",
    "  return error[0]\n",
    "\n",
    "gradient=grad(MSE_error, argnums=(2,3))\n",
    "\n",
    "def update_params(dparams,dbias,parameters,bias,lr=0.001): \n",
    "  for i in range(len(dparams)):\n",
    "    parameters[i]=parameters[i] - (lr*dparams[i])\n",
    "    bias[i] = bias[i] - (lr*dbias[i]) \n",
    "  return [parameters,bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "params=[]\n",
    "biases=[]\n",
    "params.append(np.random.randn(32,1))\n",
    "params.append(np.random.randn(1,32))\n",
    "biases.append(np.random.randn(32))\n",
    "biases.append(np.random.randn(1))\n",
    "\n",
    "#affine matrix: \n",
    "affine=jnp.array([[1,0],\n",
    "       [0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-dbd6eb14ff8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#another problem here is now to coordinate the coordinates with the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-96bf645d2267>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(affine_matrix, coordinates, feature_map, parameters, bias)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#output will contain a 784,2 output, which has been trasnformed and can be applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#to the actual image (feature_map)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformation_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffine_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-96bf645d2267>\u001b[0m in \u001b[0;36mtransformation_network\u001b[0;34m(affine_matrix, coordinates, feature_map, parameters, bias)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#This is basically saying \"perform the affine matrix, and apply it whether or not the sampling _networ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#says to do it. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maffine_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msampling_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;31m#This should be a 784,2 output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "#another problem here is now to coordinate the coordinates with the image. \n",
    "forward(affine,coordinates[2],image1,params,biases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=coordinates[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
